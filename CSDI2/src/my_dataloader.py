import pickle
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
import torch
import os

# --- é…ç½®å¸¸é‡ ---
ATTRIBUTES = ['TEM', 'RHU', 'PRS', 'WINS']
FEAT_DIM = len(ATTRIBUTES)
# å‡è®¾ä½ çš„æ•°æ®æ–‡ä»¶åå’Œè·¯å¾„
Dir_PATH = "/workspace/six_features/only-shanxi/all_six"
EARTH_RADIUS_KM = 6371.0

def haversine_distance(lat1, lon1, lat2, lon2):
    dlat = lat1 - lat2
    dlon = lon1 - lon2
    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2
    c = 2 * np.arcsin(np.sqrt(a))
    return EARTH_RADIUS_KM * c

def build_station_graph_from_csv(
    csv_path,
    k=5,
    self_loop=True,
    use_gaussian_weight=True,
):
    """
    æ ¹æ®å°ç«™ç»çº¬åº¦æ„é€ å›¾:
    - k è¿‘é‚»æ— å‘å›¾
    è¿”å›:
        edge_index  : (2, E) torch.long
        edge_weight : (E,)   torch.float
    å¦å¤–é¡ºæ‰‹è¿”å›ä¸€ä¸ªç¨ å¯† adj_npï¼Œå¤‡ç”¨ï¼ˆå¦‚æœä½ åˆ«å¤„è¦ç”¨ï¼‰ã€‚
    """
    df = pd.read_csv(csv_path)

    lats = df["lat"].values.astype(np.float64)
    lons = df["lon"].values.astype(np.float64)

    lat_rad = np.radians(lats)
    lon_rad = np.radians(lons)
    lat1 = lat_rad[:, None]
    lat2 = lat_rad[None, :]
    lon1 = lon_rad[:, None]
    lon2 = lon_rad[None, :]

    dist_mat = haversine_distance(lat1, lon1, lat2, lon2)  # (N,N)
    N = dist_mat.shape[0]

    adj = np.zeros((N, N), dtype=np.float32)

    for i in range(N):
        d = dist_mat[i].copy()
        d[i] = np.inf
        nn_idx = np.argsort(d)[:k]
        adj[i, nn_idx] = 1.0

    # å¯¹ç§°åŒ–
    adj = np.maximum(adj, adj.T)

    if self_loop:
        np.fill_diagonal(adj, 1.0)

    if use_gaussian_weight:
        d_nonzero = dist_mat[adj > 0]
        sigma = np.median(d_nonzero) if d_nonzero.size > 0 else 1.0
        sigma = max(sigma, 1e-6)
        weight = np.exp(-(dist_mat ** 2) / (2 * sigma ** 2)).astype(np.float32)
        weight[adj == 0] = 0.0
    else:
        weight = adj.copy()

    src, dst = np.nonzero(adj)
    edge_index = torch.from_numpy(np.vstack([src, dst]).astype(np.int64))  # (2,E)
    edge_weight = torch.from_numpy(weight[src, dst].astype(np.float32))    # (E,)

    return adj.astype(np.float32), edge_index, edge_weight

# --- è¾…åŠ©å‡½æ•°ï¼šè·å–å…¨éƒ¨æ•°æ®å’Œç´¢å¼•åˆ’åˆ† ---
def get_all_data_and_indices(dir_path, eval_length, train_ratio=0.7, valid_ratio=0.1):
    """
    ç°åœ¨ dir_path ä¸‹å­˜çš„æ˜¯ 4 ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶å½¢çŠ¶ (T, num_stations)ï¼Œ
    åˆ†åˆ«å¯¹åº” ATTRIBUTES é‡Œçš„ 4 ä¸ªç‰¹å¾ï¼ˆä¾‹å¦‚ TEM / RHU / PRS / WINSï¼‰ã€‚
    
    æœ¬å‡½æ•°ä¼šæŠŠå®ƒä»¬é‡æ–°ç»„ç»‡æˆï¼š
        full_data_with_nan: (T, num_stations * FEAT_DIM)
    ä¸”åˆ—é¡ºåºä¸ºï¼š [ç«™1çš„4ç»´, ç«™2çš„4ç»´, ..., ç«™Nçš„4ç»´]ï¼Œ
    è¿™æ ·ä¸åŸæ¥â€œæ¯ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ªç«™ç‚¹ã€åˆ—æ˜¯4ä¸ªç‰¹å¾â€çš„æ•ˆæœä¸€è‡´ã€‚
    """

    # 1. å…ˆæŠŠæ¯ä¸ªç‰¹å¾æ–‡ä»¶è¯»è¿›æ¥ï¼š attr_name -> DataFrame(T, num_stations)
    feature_dfs = {}  # { 'TEM': df_tem, 'RHU': df_rhu, ... }

    for fname in os.listdir(dir_path):
        if not fname.endswith(".csv"):
            continue
        fpath = os.path.join(dir_path, fname)
        for attr in ATTRIBUTES:
            # ç®€å•åŒ¹é…ï¼šæ–‡ä»¶åé‡ŒåŒ…å«ç‰¹å¾åï¼Œæ¯”å¦‚ TEM_xxx.csvã€RHU.csv ç­‰
            if attr in fname:
                df = pd.read_csv(fpath, index_col=0, parse_dates=True)
                feature_dfs[attr] = df
                break

    # ç®€å•é˜²å‘†ï¼šç¡®è®¤ 4 ä¸ªç‰¹å¾éƒ½æ‰¾åˆ°äº†
    if len(feature_dfs) != len(ATTRIBUTES):
        raise ValueError(f"åœ¨ç›®å½• {dir_path} ä¸‹æ²¡æœ‰æ‰¾åˆ°æ‰€æœ‰ç‰¹å¾æ–‡ä»¶ï¼ŒæœŸæœ› {ATTRIBUTES}ï¼Œå®é™… {list(feature_dfs.keys())}")

    # 2. ç»Ÿä¸€æ—¶é—´ç´¢å¼•å’Œå°ç«™åˆ—é¡ºåº
    ref_attr = ATTRIBUTES[0]
    ref_df = feature_dfs[ref_attr]
    full_datetime_index = ref_df.index.values  # æ—¶é—´è½´
    station_ids = list(ref_df.columns)         # å°ç«™é¡ºåº
    T = len(full_datetime_index)
    num_stations = len(station_ids)

    # 3. æŒ‰â€œç«™ç‚¹ä¼˜å…ˆâ€çš„æ–¹å¼é‡æ–°æ‹¼æ¥ï¼š [ç«™1çš„4ç»´, ç«™2çš„4ç»´, ...]
    full_data_list = []
    full_mask_list = []

    for sid in station_ids:
        station_feat_list = []
        station_mask_list = []
        for attr in ATTRIBUTES:
            df_attr = feature_dfs[attr]

            # ä¿è¯æ—¶é—´å’Œåˆ—é¡ºåºä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´ä½ å°±å¾—å…ˆåœ¨å¤–é¢æ¸…æ´—æ•°æ®äº†
            series = df_attr[sid]                     # (T,)
            values = series.values.reshape(T, 1)      # (T,1)
            mask = (~series.isna()).astype(np.float32).values.reshape(T, 1)  # 1=è§‚æµ‹,0=NaN

            station_feat_list.append(values)  # è¯¥ç«™çš„ä¸€ä¸ªç‰¹å¾
            station_mask_list.append(mask)

        # è¯¥ç«™çš„ 4 ç»´ç‰¹å¾ï¼š (T, FEAT_DIM)
        station_data = np.concatenate(station_feat_list, axis=1)
        station_c_mask = np.concatenate(station_mask_list, axis=1)

        full_data_list.append(station_data)
        full_mask_list.append(station_c_mask)

    # æ²¿ç‰¹å¾ç»´æ‹¼æ¥æ‰€æœ‰ç«™ç‚¹ â†’ (T, num_stations * FEAT_DIM)
    full_data_with_nan = np.concatenate(full_data_list, axis=1)
    full_c_mask = np.concatenate(full_mask_list, axis=1)

    K_total = full_data_with_nan.shape[1]
    print(f"[INFO] Loaded {num_stations} stations Ã— {FEAT_DIM} features -> feature dim = {K_total}")

    # 4. æ»‘åŠ¨çª—å£ç´¢å¼•ï¼ˆè¿™éƒ¨åˆ†ä¿æŒä¸å˜ï¼‰
    N = T - eval_length + 1
    all_indices = np.arange(N)
    n_train = int(N * train_ratio)
    n_valid = int(N * valid_ratio)

    train_indices = all_indices[:n_train]
    valid_indices = all_indices[n_train:n_train + n_valid]
    test_indices = all_indices[n_train + n_valid:]

    train_start_idx = train_indices[0] if len(train_indices) > 0 else -1
    valid_start_idx = valid_indices[0] if len(valid_indices) > 0 else -1
    test_start_idx = test_indices[0] if len(test_indices) > 0 else -1

    return (
        full_data_with_nan,
        full_c_mask,
        full_datetime_index,
        train_indices,
        valid_indices,
        test_indices,
        train_start_idx,
        valid_start_idx,
        test_start_idx,
    )


# --- è¾…åŠ©å‡½æ•°ï¼šä»…åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®— Mean/Std ---
def calculate_train_mean_std(full_data_with_nan, full_c_mask, train_indices, eval_length):
    """
    åœ¨å¤šå°ç«™åœºæ™¯ä¸‹è®¡ç®—è®­ç»ƒé›†ä¸Šçš„å‡å€¼å’Œæ–¹å·®ã€‚

    - full_data_with_nan: (T, num_stations * FEAT_DIM)
    - full_c_mask       : åŒå½¢çŠ¶ï¼Œ1=è§‚æµ‹, 0=åŸå§‹ç¼ºå¤±
    - è¿”å›:
        mean: (FEAT_DIM,)ï¼Œæ¯ä¸ªåŸå§‹æ°”è±¡ç‰¹å¾ä¸€ä¸ªå‡å€¼ï¼ˆè·¨æ‰€æœ‰ç«™ç‚¹ + æ—¶é—´ï¼‰
        std : (FEAT_DIM,)
    ç»“æœä¼šä¿å­˜åœ¨ STATS_CACHE_PATHï¼Œä¸‹æ¬¡ä¼˜å…ˆä»ç£ç›˜åŠ è½½ã€‚
    """
    STATS_CACHE_PATH = "/workspace/CSDI2/Cache/train_mean_std_multi_station.pkl"
    # ---------- å…ˆå°è¯•ä»ç¼“å­˜è¯»å– ----------
    if os.path.exists(STATS_CACHE_PATH):
        try:
            with open(STATS_CACHE_PATH, "rb") as f:
                cache = pickle.load(f)
            mean = cache["mean"]
            std = cache["std"]
            print(f"[INFO] Loaded cached train mean/std from {STATS_CACHE_PATH}")
            return mean, std
        except Exception as e:
            print(f"[WARN] Failed to load mean/std cache: {e}, recomputing...")

    print("Calculating mean and std from training set (multi-station)...")

    # æ”¶é›†æ‰€æœ‰è®­ç»ƒçª—å£ä¸­çš„è§‚æµ‹å€¼
    train_values = []
    train_masks = []

    for start_index in train_indices:
        seq = full_data_with_nan[start_index: start_index + eval_length]   # (L, K_total)
        mask = full_c_mask[start_index: start_index + eval_length]        # (L, K_total)

        train_values.append(seq)
        train_masks.append(mask)

    # æ‹¼æˆ (M, K_total)
    tmp_values = np.concatenate(train_values, axis=0)
    tmp_masks = np.concatenate(train_masks, axis=0)

    M, K_total = tmp_values.shape
    if K_total % FEAT_DIM != 0:
        raise ValueError(
            f"K_total={K_total} æ— æ³•è¢« FEAT_DIM={FEAT_DIM} æ•´é™¤ï¼Œ"
            f"è¯·æ£€æŸ¥ full_data_with_nan çš„åˆ—å¸ƒå±€æ˜¯å¦æ˜¯ [ç«™1çš„{FEAT_DIM}ç»´, ç«™2çš„{FEAT_DIM}ç»´, ...]"
        )

    num_stations = K_total // FEAT_DIM

    # é‡æ–°æ•´ç†æˆ (M, num_stations, FEAT_DIM)
    tmp_values = tmp_values.reshape(M, num_stations, FEAT_DIM)
    tmp_masks = tmp_masks.reshape(M, num_stations, FEAT_DIM)

    mean = np.zeros(FEAT_DIM, dtype=np.float32)
    std = np.zeros(FEAT_DIM, dtype=np.float32)

    # å¯¹æ¯ä¸ªâ€œåŸå§‹ç‰¹å¾â€ç»´åº¦ï¼ˆTEM/RHU/PRS/WINSï¼‰åšç»Ÿè®¡ï¼Œè·¨æ‰€æœ‰ç«™ç‚¹ + æ—¶é—´
    for k in range(FEAT_DIM):
        # å–å‡ºè¯¥ç‰¹å¾åœ¨æ‰€æœ‰ç«™ç‚¹çš„è§‚æµ‹å€¼
        c_data = tmp_values[:, :, k][tmp_masks[:, :, k] == 1]

        if c_data.size == 0:
            mean[k] = 0.0
            std[k] = 1.0
        else:
            m = c_data.mean()
            s = c_data.std()
            mean[k] = m
            std[k] = s if s > 1e-6 else 1.0

    # ---------- æŒä¹…åŒ–åˆ°ç£ç›˜ ----------
    try:
        with open(STATS_CACHE_PATH, "wb") as f:
            pickle.dump({"mean": mean, "std": std}, f)
        print(f"[INFO] Saved train mean/std to {STATS_CACHE_PATH}")
    except Exception as e:
        print(f"[WARN] Failed to save mean/std cache: {e}")

    return mean, std

# è¿™æ˜¯æ’è¡¥ä»»åŠ¡å¦ä¸€ä¸ªåˆ›é€ éªŒè¯/æµ‹è¯•é›†çš„æ©ç çš„å‡½æ•°ï¼Œéšæœºç¼ºå¤±å’Œè¿ç»­å—æŸå¤±å…±å­˜
def create_full_gt_mask2(
    full_c_mask,
    row_indices,
    missing_ratio,
    block_prob=0.3,      # è¿ç»­å—ç¼ºå¤±ç‚¹æ•°å æ€»ç¼ºå¤±ç‚¹æ•°çš„å¤§çº¦æ¯”ä¾‹ï¼ˆ0~1ï¼‰
    max_block_len=4,    # è¿ç»­å—çš„æœ€å¤§é•¿åº¦ï¼ˆæ²¿æ—¶é—´è½´ / è¡Œæ–¹å‘ï¼‰
    seed=None,
):
    """
    åœ¨ `row_indices` æŒ‡å®šçš„è¡ŒèŒƒå›´å†…ã€é’ˆå¯¹åŸå§‹è§‚æµ‹ç‚¹(=1)ï¼Œç”Ÿæˆæ··åˆç¼ºå¤± Maskï¼š
    - ä¸€éƒ¨åˆ†ç¼ºå¤±æ¥è‡ªæ²¿æ—¶é—´è½´çš„ã€Œè¿ç»­å—ã€
    - å‰©ä½™ç¼ºå¤±æ¥è‡ªã€Œéšæœºæ•£ç‚¹ã€
    
    å‚æ•°:
        full_c_mask (np.ndarray): åŸå§‹è§‚æµ‹ Mask (1=è§‚æµ‹, 0=åŸå§‹ç¼ºå¤±)ã€‚å½¢çŠ¶ (H, W)ã€‚
        row_indices (list/np.ndarray): éœ€è¦åœ¨å…¶ä¸­ç”Ÿæˆæ–°ç¼ºå¤±çš„è¡Œä¸‹æ ‡ï¼ˆå¦‚ [6000, 6001, ...]ï¼‰ã€‚
        missing_ratio (float): ç›®æ ‡ç¼ºå¤±æ¯”ä¾‹ï¼ˆç›¸å¯¹äº row_indices èŒƒå›´å†…ã€è§‚æµ‹ç‚¹=1 çš„æ•°é‡ï¼‰ã€‚
        block_prob (float): å¤§çº¦æœ‰å¤šå°‘æ¯”ä¾‹çš„ç¼ºå¤±ç‚¹é€šè¿‡ã€Œè¿ç»­å—ã€æ–¹å¼ç”Ÿæˆï¼ŒèŒƒå›´ [0,1]ã€‚
                            ä¾‹å¦‚ 0.7 è¡¨ç¤ºçº¦ 70% ç¼ºå¤±ç‚¹å±äºè¿ç»­å—ï¼Œå…¶ä½™ 30% æ˜¯éšæœºç‚¹ã€‚
        max_block_len (int): è¿ç»­å—çš„æœ€å¤§é•¿åº¦ (>=1)ã€‚å®é™…å—é•¿ä¼šåœ¨ [1, max_block_len] å†…éšæœºé‡‡æ ·ã€‚
        seed (int, optional): éšæœºç§å­ã€‚

    è¿”å›:
        np.ndarray: æ–°çš„ Maskï¼Œå½¢çŠ¶ä¸ full_c_mask ç›¸åŒã€‚
    """
    # ---------- åŸºæœ¬æ£€æŸ¥ ----------
    if not (0.0 <= float(missing_ratio) <= 1.0):
        raise ValueError(f"missing_ratio å¿…é¡»åœ¨ [0,1]ï¼Œç»™åˆ° {missing_ratio}")

    if not (0.0 <= float(block_prob) <= 1.0):
        raise ValueError(f"block_prob å¿…é¡»åœ¨ [0,1]ï¼Œç»™åˆ° {block_prob}")

    if not (isinstance(max_block_len, int) and max_block_len >= 1):
        raise ValueError(f"max_block_len å¿…é¡»æ˜¯ >=1 çš„æ•´æ•°ï¼Œç»™åˆ° {max_block_len}")

    masks = full_c_mask.copy()
    H, W = masks.shape

    # è§„èŒƒ row_indices
    if isinstance(row_indices, list):
        row_indices = np.array(row_indices, dtype=int)
    else:
        row_indices = np.asarray(row_indices, dtype=int)

    if row_indices.size == 0:
        raise ValueError("row_indices ä¸ºç©ºã€‚")

    if row_indices.min() < 0 or row_indices.max() >= H:
        raise ValueError(f"row_indices æœ‰è¶Šç•Œï¼šåˆæ³•èŒƒå›´ [0, {H-1}]")

    if seed is not None:
        np.random.seed(seed)

    # åªåœ¨ row_indices è¿™ä¸€å­åŒºé—´å†…æ“ä½œ
    sub_mask = masks[row_indices, :]          # (R, W)
    sub_obs = (sub_mask == 1)                 # True = åŸå§‹è§‚æµ‹ç‚¹ï¼Œå¯æŒ–

    num_total_obs = sub_obs.sum()
    if num_total_obs == 0:
        raise RuntimeError("æŒ‡å®šè¡ŒèŒƒå›´å†…æ²¡æœ‰å¯ç”¨çš„åŸå§‹è§‚æµ‹ç‚¹(=1)ï¼Œæ— æ³•ç”Ÿæˆç¼ºå¤±ã€‚")

    # æ€»ç¼ºå¤±ç‚¹æ•°
    num_to_miss = int(round(num_total_obs * missing_ratio))
    if num_to_miss <= 0:
        raise RuntimeError(
            f"ç›®æ ‡ç¼ºå¤±æ•°ä¸º {num_to_miss}ï¼ˆå¯èƒ½æ˜¯ missing_ratio å¤ªå°æˆ–å¯ç”¨è§‚æµ‹ç‚¹è¿‡å°‘ï¼‰ã€‚"
        )

    # å…¶ä¸­ä¸€éƒ¨åˆ†ç”¨ã€Œè¿ç»­å—ã€äº§ç”Ÿ
    num_block_points = int(round(num_to_miss * block_prob))
    # å‰©ä½™éƒ¨åˆ†ç”¨ã€Œéšæœºç‚¹ã€äº§ç”Ÿ
    num_random_points = num_to_miss - num_block_points

    # åœ¨å­åŒºé—´é‡Œç»´æŠ¤ä¸€ä¸ªã€Œè¿˜å¯æŒ–ã€çš„å¸ƒå°”çŸ©é˜µ
    placeable = sub_obs.copy()   # True è¡¨ç¤ºå½“å‰ä»å¯æŒ–æ‰

    # ç”¨äºè®°å½•æ‰€æœ‰é€‰ä¸­çš„ (sub_row_offset, col) åæ ‡
    chosen_rc = []

    # ---------- è¾…åŠ©å‡½æ•°ï¼šæ‰¾åˆ° True çš„è¿ç»­åŒºé—´ ----------
    def find_true_runs(x):
        """
        åœ¨ 1D bool æ•°ç»„ x ä¸­æ‰¾åˆ°æ‰€æœ‰è¿ç»­ True çš„åŒºé—´ [s, e]ï¼ˆé—­åŒºé—´ï¼‰
        è¿”å›: list of (s, e)
        """
        x = np.asarray(x, dtype=bool)
        if x.size == 0:
            return []

        # æ‰¾åˆ° Trueâ†’False çš„è¾¹ç•Œ å’Œ Falseâ†’True çš„è¾¹ç•Œ
        # run_starts = False->True çš„ä½ç½®
        # run_ends   = True->False çš„ä½ç½®
        diff = np.diff(x.astype(int))

        run_starts = np.where(diff == 1)[0] + 1
        run_ends = np.where(diff == -1)[0]

        # å¦‚æœç¬¬ä¸€ä¸ªå…ƒç´ å°±æ˜¯ Trueï¼Œåˆ™å®ƒæ˜¯ä¸€ä¸ªè¿ç»­æ®µçš„å¼€å¤´
        if x[0]:
            run_starts = np.concatenate(([0], run_starts))

        # å¦‚æœæœ€åä¸€ä¸ªå…ƒç´ æ˜¯ Trueï¼Œåˆ™å®ƒæ˜¯ä¸€ä¸ªè¿ç»­æ®µçš„ç»“å°¾
        if x[-1]:
            run_ends = np.concatenate((run_ends, [x.size - 1]))

        # ç»„è£…æˆåŒºé—´
        runs = list(zip(run_starts, run_ends))
        return runs

    # ---------- ç¬¬ä¸€æ­¥ï¼šæ”¾ç½®è¿ç»­å— ----------
    def place_blocks(placeable, target_points, max_block_len):
        """
        åœ¨ 2D çš„ placeable(True/False) ä¸­æ”¾ç½®è‹¥å¹²è¿ç»­å—ï¼Œç›´åˆ°
        - æ”¾å¤Ÿ target_pointsï¼Œæˆ–è€…
        - å·²æ— è¶³å¤Ÿç©ºé—´
        è¿”å›ï¼š
            chosen_list: [(sub_row, col), ...]
            placeable   : æ›´æ–°åçš„ placeable
        """
        R, C = placeable.shape
        chosen_list = []
        remain = target_points

        while remain > 0 and placeable.any():
            # æ‰¾å‡ºå½“å‰è¿˜æœ‰ True çš„åˆ—
            cols = np.where(placeable.any(axis=0))[0]
            if cols.size == 0:
                break

            # éšæœºæŒ‘ä¸€åˆ—
            j = np.random.choice(cols)
            col_vec = placeable[:, j]

            # è¯¥åˆ—ä¸­è¿ç»­ True æ®µ
            runs = find_true_runs(col_vec)
            # è¿‡æ»¤æ‰é•¿åº¦ä¸º 0 çš„æ®µ
            runs = [(s, e) for (s, e) in runs if (e - s + 1) > 0]
            if not runs:
                # è¿™ä¸€åˆ—æ²¡æ³•æ”¾ï¼Œç›´æ¥æŠŠè¯¥åˆ—æ ‡è®°ä¸ºä¸å¯ç”¨å†ç»§ç»­
                placeable[:, j] = False
                continue

            # å½“å‰åˆ—ä¸­èƒ½æ”¾çš„æœ€é•¿ run é•¿åº¦
            max_len_col = max(e - s + 1 for (s, e) in runs)

            # å¦‚æœè¿™ä¸€åˆ—æ•´ä½“æœ€é•¿ run ä¹Ÿä¸º 0ï¼Œåˆ™æ”¾å¼ƒè¿™ä¸€åˆ—
            if max_len_col <= 0:
                placeable[:, j] = False
                continue

            # è¿™æ¬¡å°è¯•çš„å—é•¿ Lï¼šåœ¨ 1 ~ min(max_block_len, max_len_col, remain) å†…éšæœºå–
            L_upper = min(max_block_len, max_len_col, remain)
            if L_upper <= 0:
                break
            L = np.random.randint(1, L_upper + 1)  # [1, L_upper]

            # å†åœ¨ runs ä¸­æŒ‘å‡ºèƒ½å®¹çº³ L çš„ run
            candidate_runs = [(s, e) for (s, e) in runs if (e - s + 1) >= L]
            if not candidate_runs:
                # å¦‚æœå› ä¸º L è¿‡å¤§è€Œæ‰¾ä¸åˆ°åˆé€‚ runï¼Œé‚£å°±ç›´æ¥ç¼©åˆ°è¯¥åˆ—æœ€å¤§ run é•¿åº¦
                L = min(max_len_col, remain)
                candidate_runs = [(s, e) for (s, e) in runs if (e - s + 1) >= L]
                if not candidate_runs:
                    placeable[:, j] = False
                    continue

            # éšæœºé€‰ä¸€ä¸ª run
            s, e = candidate_runs[np.random.randint(len(candidate_runs))]
            # åœ¨ run å†…éšæœºé€‰æ‹©èµ·ç‚¹
            start = np.random.randint(s, e - L + 2)  # [s, e-L+1]
            end = start + L - 1

            # æ ‡è®°è¿™ä¸€æ®µä¸ºå·²ä½¿ç”¨ï¼Œå¹¶è®°å½•åæ ‡
            placeable[start:end + 1, j] = False
            for r in range(start, end + 1):
                chosen_list.append((r, j))
            remain -= L

        return chosen_list, placeable

    # è¿ç»­å—ç¼ºå¤±éƒ¨åˆ†
    if num_block_points > 0:
        block_chosen, placeable = place_blocks(
            placeable=placeable,
            target_points=num_block_points,
            max_block_len=max_block_len,
        )
        chosen_rc.extend(block_chosen)

    # ---------- ç¬¬äºŒæ­¥ï¼šéšæœºç‚¹è¡¥é½å‰©ä½™ ----------
    # å‰©ä½™éœ€è¦ç¼ºå¤±çš„ç‚¹æ•°ï¼ˆå¦‚æœå—æ²¡æ”¾æ»¡ï¼Œä¼šå¢åŠ éšæœºéƒ¨åˆ†ï¼‰
    used_block_points = len(chosen_rc)
    remain_random = num_to_miss - used_block_points
    if remain_random > 0:
        # å½“å‰ä»å¯æŒ–çš„å€™é€‰ç‚¹
        candidates = np.column_stack(np.where(placeable))  # (N, 2) -> (sub_row, col)
        if candidates.size == 0:
            # æ”¾ä¸æ»¡å°±åªèƒ½æ¥å—ã€Œå®é™…ç¼ºå¤±ç‚¹æ•° < ç›®æ ‡ç¼ºå¤±ç‚¹æ•°ã€
            # è¿™é‡Œä¸æŠ¥é”™ï¼Œåªæ˜¯è­¦å‘Š
            print(
                f"[WARN] ç›®æ ‡ç¼ºå¤±ç‚¹æ•°={num_to_miss}ï¼Œ"
                f"å®é™…æœ€å¤šåªèƒ½æ”¾ç½® {used_block_points} ä¸ªï¼ˆè¿ç»­å—+éšæœºç‚¹ï¼‰ã€‚"
            )
        else:
            take = min(remain_random, candidates.shape[0])
            idx = np.random.choice(np.arange(candidates.shape[0]), take, replace=False)
            extra = candidates[idx]
            for r, j in extra:
                chosen_rc.append((int(r), int(j)))

    if not chosen_rc:
        raise RuntimeError("æ— æ³•æ”¾ç½®ä»»ä½•ç¼ºå¤±å—/ç‚¹ï¼ˆå¯èƒ½å¯ç”¨è§‚æµ‹æ®µè¿‡çŸ­/è¿‡å°‘ï¼‰ã€‚")

    # ---------- æ˜ å°„å›åŸå§‹å…¨å±€è¡Œåˆ—ï¼Œå¹¶ç½® 0 ----------
    chosen_rc = np.array(chosen_rc, dtype=int)
    sub_rows = chosen_rc[:, 0]  # ç›¸å¯¹äº row_indices çš„åç§»
    cols = chosen_rc[:, 1]

    global_rows = row_indices[sub_rows]
    global_cols = cols

    masks[global_rows, global_cols] = 0
    return masks

# è¿™æ˜¯æ’è¡¥ä»»åŠ¡æ„é€ gt_maskçš„å‡½æ•°
def create_full_gt_mask(full_c_mask,row_indices,missing_ratio,
    seed=None,
    mode="random",                # "random"ï¼ˆåŸé€»è¾‘ï¼‰æˆ– "block"ï¼ˆè¿ç»­å—ï¼‰
    block_len=None,               # å›ºå®šå—é•¿ï¼ˆæ­£æ•´æ•°ï¼‰ï¼›ä¸ block_len_range äºŒé€‰ä¸€
    block_len_range=None,         # (Lmin, Lmax) ï¼›å«ç«¯ç‚¹ï¼Œå‡åŒ€éšæœº
    per_col=True                  # ç¼ºå¤±é…é¢æ˜¯å¦æŒ‰å„åˆ—å¯ç”¨è§‚æµ‹ç‚¹æ¯”ä¾‹åˆ†æ‘Š
):
    """
    åœ¨ `row_indices` æŒ‡å®šçš„è¡ŒèŒƒå›´å†…ã€é’ˆå¯¹åŸå§‹è§‚æµ‹ç‚¹(=1)ï¼Œç”ŸæˆéªŒè¯/æµ‹è¯•ç¼ºå¤± Maskã€‚
    - mode="random"ï¼šéšæœºç‚¹çŠ¶ç¼ºå¤±ï¼ˆä¸ä½ åŸé€»è¾‘ä¸€è‡´ï¼‰
    - mode="block" ï¼šç”Ÿæˆæ²¿æ—¶é—´è½´ï¼ˆè¡Œæ–¹å‘ï¼‰çš„è¿ç»­ç¼ºå¤±å—

    å‚æ•°:
        full_c_mask (np.ndarray): åŸå§‹è§‚æµ‹ Mask (1=è§‚æµ‹, 0=åŸå§‹ç¼ºå¤±)ã€‚å½¢çŠ¶ (H, W)ã€‚
        row_indices (list/np.ndarray): éœ€è¦åœ¨å…¶ä¸­ç”Ÿæˆæ–°ç¼ºå¤±çš„è¡Œä¸‹æ ‡ï¼ˆå¦‚ [6000, 6001, ...]ï¼‰ã€‚
        missing_ratio (float): ç›®æ ‡ç¼ºå¤±æ¯”ä¾‹ï¼ˆç›¸å¯¹äº row_indices èŒƒå›´å†…ã€è§‚æµ‹ç‚¹=1 çš„æ•°é‡ï¼‰ã€‚
        seed (int, optional): éšæœºç§å­ã€‚
        mode (str): "random" æˆ– "block"ã€‚
        block_len (int): è¿ç»­å—å›ºå®šé•¿åº¦ï¼›ä¸ block_len_range äºŒé€‰ä¸€ã€‚
        block_len_range (tuple): (Lmin, Lmax)ï¼›å«ç«¯ç‚¹ï¼Œå‡åŒ€æŠ½æ ·ã€‚
        per_col (bool): æ˜¯å¦æŒ‰å„åˆ—çš„â€œå¯ç”¨è§‚æµ‹ç‚¹æ•°â€æŒ‰æ¯”ä¾‹åˆ†æ‘Šç¼ºå¤±é…é¢ã€‚

    è¿”å›:
        np.ndarray: æ–°çš„ Maskï¼Œå½¢çŠ¶ä¸ full_c_mask ç›¸åŒã€‚
    """
    # ---------- åŸºæœ¬æ£€æŸ¥ ----------
    if not (0.0 <= float(missing_ratio) <= 1.0):
        raise ValueError(f"missing_ratio å¿…é¡»åœ¨ [0,1]ï¼Œç»™åˆ° {missing_ratio}")

    masks = full_c_mask.copy()
    H, W = masks.shape

    if isinstance(row_indices, list):
        row_indices = np.array(row_indices, dtype=int)
    else:
        row_indices = np.asarray(row_indices, dtype=int)

    if row_indices.size == 0:
        raise ValueError("row_indices ä¸ºç©ºã€‚")

    if row_indices.min() < 0 or row_indices.max() >= H:
        raise ValueError(f"row_indices æœ‰è¶Šç•Œï¼šåˆæ³•èŒƒå›´ [0, {H-1}]")

    if seed is not None:
        np.random.seed(seed)

    # æ‰å¹³åŒ–è¾…åŠ©
    masks_flat = masks.reshape(-1)

    # ç›®æ ‡å€™é€‰ï¼šrow_indices Ã— å…¨åˆ—
    R, C = np.meshgrid(row_indices, np.arange(W), indexing='ij')
    target_flat_indices_candidates = (R * W + C).ravel()

    # åªå…è®¸å¯¹åŸå§‹è§‚æµ‹ç‚¹=1åŠ¨åˆ€
    target_obs_indices = target_flat_indices_candidates[masks_flat[target_flat_indices_candidates] == 1]
    num_total_obs = target_obs_indices.size

    if num_total_obs == 0:
        raise RuntimeError("æŒ‡å®šè¡ŒèŒƒå›´å†…æ²¡æœ‰å¯ç”¨çš„åŸå§‹è§‚æµ‹ç‚¹(=1)ï¼Œæ— æ³•ç”Ÿæˆç¼ºå¤±ã€‚")

    num_to_miss = int(round(num_total_obs * missing_ratio))
    if num_to_miss <= 0:
        raise RuntimeError(
            f"ç›®æ ‡ç¼ºå¤±æ•°ä¸º {num_to_miss}ï¼ˆå¯èƒ½æ˜¯ missing_ratio å¤ªå°æˆ–å¯ç”¨è§‚æµ‹ç‚¹è¿‡å°‘ï¼‰ã€‚"
        )

    # ---------- æ¨¡å¼ä¸€ï¼šéšæœºç‚¹çŠ¶ï¼ˆä¸ä½ çš„æ—§é€»è¾‘ä¸€è‡´ï¼‰ ----------
    if mode == "random":
        miss_indices = np.random.choice(target_obs_indices, num_to_miss, replace=False)
        masks_flat[miss_indices] = 0
        return masks_flat.reshape(masks.shape)

    # ---------- æ¨¡å¼äºŒï¼šè¿ç»­å— ----------
    if mode != "block":
        raise ValueError(f"mode åªèƒ½æ˜¯ 'random' æˆ– 'block'ï¼Œç»™åˆ° {mode}")

    # å—é•¿é…ç½®æ£€æŸ¥
    if (block_len is None) == (block_len_range is None):
        raise ValueError("block_len ä¸ block_len_range éœ€äºŒé€‰ä¸€ã€‚")
    if block_len is not None:
        if not (isinstance(block_len, int) and block_len >= 1):
            raise ValueError("block_len å¿…é¡»æ˜¯ >=1 çš„æ•´æ•°")
        def sample_block_len():
            return block_len
    else:
        Lmin, Lmax = block_len_range
        if not (isinstance(Lmin, int) and isinstance(Lmax, int) and 1 <= Lmin <= Lmax):
            raise ValueError("block_len_range éœ€ä¸º (Lmin, Lmax) ä¸” 1 <= Lmin <= Lmaxï¼Œæ•´æ•°")
        def sample_block_len():
            return np.random.randint(Lmin, Lmax + 1)

    # æŠŠ row_indices å¯¹åº”çš„å­åŒºæ®µæ‹¿å‡ºæ¥ï¼Œä¾¿äºåˆ—å†…æ“ä½œ
    sub_mask = masks[row_indices, :]        # å½¢çŠ¶ (R, W)
    sub_obs  = (sub_mask == 1)              # True è¡¨ç¤ºå¯æ”¾ç½®ç¼ºå¤±

    # å„åˆ—å¯ç”¨è§‚æµ‹ç‚¹ç»Ÿè®¡
    # col_obs_countsï¼šå¯¹sub_obsæŒ‰è¡Œæ±‚å’Œ
    col_obs_counts = sub_obs.sum(axis=0)    # (W,)
    # total_obs_in_subï¼šå¯¹col_obs_countsæ±‚å’Œï¼Œç›¸å½“äºå¯¹sub_obsæ•´ä½“æ±‚å’Œ
    total_obs_in_sub = col_obs_counts.sum()
    assert total_obs_in_sub == num_total_obs

    # ä¸ºé¿å…é‡å¤/é‡å ï¼Œåœ¨ sub é‡Œç»´æŠ¤ä¸€ä¸ªâ€œè¿˜èƒ½æ”¾â€çš„å·¥ä½œå‰¯æœ¬
    placeable = sub_obs.copy()  # True=å½“å‰ä»å¯æ”¾ç½®ç¼ºå¤±

    # åˆ—é…é¢ï¼šæŒ‰æ¯”ä¾‹åˆ†æ‘Š or ç»Ÿä¸€æ± 
    if per_col:# è®©ç¼ºå¤±ç‚¹åœ¨æ¯ä¸ªåˆ—ä¸ŠæŒ‰åŸæ¥è§‚æµ‹ç‚¹çš„å¤šå°‘æ¯”ä¾‹åˆ†æ‘Šã€‚
        # æŒ‰åˆ—å æ¯”è®¡ç®—é…é¢ï¼Œå¹¶ç”¨â€œæœ€å¤§ä½™æ•°æ³•â€æ”¶å°¾ï¼Œä¿è¯æ€»å’Œç­‰äº num_to_miss
        # egï¼š
        #   col_obs_counts = [100, 200, 700]   # æ¯åˆ—è§‚æµ‹ç‚¹æ•°
        #   num_to_miss = 100   # æƒ³æ€»å…±ç¼ºå¤±100ä¸ªç‚¹
        #   raw_alloc = (col_obs_counts / total_obs_in_sub) * num_to_miss = [10.0, 20.0, 70.0]
        #   col_quota = floor(raw_alloc) = [10, 20, 70]
        raw_alloc = (col_obs_counts / (total_obs_in_sub + 1e-12)) * num_to_miss
        col_quota = np.floor(raw_alloc).astype(int)
        remainder = num_to_miss - col_quota.sum()
        if remainder > 0:# å¦‚æœæ€»å’Œæ¯”ç›®æ ‡å°‘ï¼ˆæ¯”å¦‚å› ä¸ºå°æ•°éƒ¨åˆ†è¢«ç æ‰ï¼‰
            # æŠŠå°æ•°éƒ¨åˆ†æœ€å¤§çš„åˆ—ä¼˜å…ˆè¡¥é½
            frac = raw_alloc - col_quota # è®¡ç®—æ¯åˆ—è¢«ç æ‰çš„å°æ•°éƒ¨åˆ†ï¼š
            order = np.argsort(-frac)  # é™åº
            # å‡å¦‚ raw_alloc = [33.4, 33.3, 33.3]ï¼Œæ€»å’Œ=100ä½†å–æ•´å [33,33,33]=99ï¼Œè¿˜ç¼º1ä¸ªã€‚
            # å°±æŠŠå°æ•°æœ€å¤§çš„ä¸€åˆ—ï¼ˆç¬¬0åˆ—ï¼‰å†+1 â†’ [34,33,33]ã€‚
            for idx in order[:remainder]: # ç„¶åæŒ‰ä»å¤§åˆ°å°æ’åºï¼Œç”¨â€œæœ€å¤§ä½™æ•°æ³•â€è¡¥é½ï¼š
                col_quota[idx] += 1
    else:
        # ä¸æŒ‰åˆ—åˆ†æ‘Šï¼šåé¢ç»Ÿä¸€åœ¨æ•´å¼  sub ä¸Šæ”¾
        col_quota = np.zeros(W, dtype=int)
        col_quota[0] = num_to_miss  # å…¨éƒ¨é…é¢å…ˆæ”¾ç¬¬ä¸€åˆ—çš„åä¹‰ä¸Šï¼Œéšåâ€œè·¨åˆ—â€å¤„ç†

    # ç»Ÿè®¡æœ€ç»ˆè¦ç½®é›¶çš„ (row_offset, col) åæ ‡é›†åˆ
    chosen_rc = []

    def pick_blocks_in_one_column(col, quota, placeable_col):
        """
        ä½œç”¨ï¼šåœ¨å•åˆ— (é•¿åº¦ = len(row_indices)) çš„ placeable_col(True/False) ä¸­æ”¾ç½® quota ä¸ªç¼ºå¤±ç‚¹ï¼Œä»¥è¿ç»­å—å½¢å¼ä¼˜å…ˆï¼›è‹¥å—æ”¾ä¸æ»¡ï¼Œå›é€€ä¸ºç‚¹çŠ¶éšæœºè¡¥é½ã€‚
        è¿”å›ï¼šé€‰ä¸­çš„è¡Œä¸‹æ ‡ listï¼ˆç›¸å¯¹äº row_indices çš„åç§»ï¼‰
        """
        if quota <= 0:
            return []

        # å·¥ä½œå‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹å¤–å±‚
        plc = placeable_col.copy()
        picked = 0
        chosen_rows = []

        # è¾…åŠ©ï¼šæ‰¾ plc==True çš„è¿ç»­æ®µ [s,e]ï¼ˆé—­åŒºé—´ï¼‰

        def find_true_runs(x):# æ‰¾åˆ°æ‰€æœ‰è¿ç»­çš„ True åŒºé—´ï¼ˆå¯ç”¨äºæ”¾ç¼ºå¤±çš„åŒºåŸŸï¼‰
            # x: 1D bool
            if x.size == 0: return []
            dx = np.diff(x.astype(np.int8))
            # run starts: where x goes 0->1
            starts = np.where((np.concatenate(([x[0]], dx == 1)) & x))[0]
            # run ends: where x goes 1->0
            ends = np.where((np.concatenate(((dx == -1), [x[-1]])) & x))[0]
            # ç»„è£…
            runs = list(zip(starts, ends))
            return runs

        # å…ˆå°½é‡ç”¨å—æ”¾
        while picked < quota:
            runs = find_true_runs(plc)
            if not runs:
                break  # å·²æ— å¯æ”¾åŒºåŸŸ

            # åœ¨æœ‰è¶³å¤Ÿé•¿åº¦å®¹çº³â€œè‡³å°‘ 1â€çš„ run ä¸­å°è¯•
            # æˆ‘ä»¬æ¯æ¬¡æŠ½ä¸€ä¸ªå—é•¿ï¼Œç„¶ååœ¨èƒ½å®¹çº³è¯¥å—çš„ runs é‡ŒéšæœºæŒ‘ä¸€ä¸ª runï¼Œå†åœ¨ run é‡ŒéšæœºæŒ‘èµ·ç‚¹
            L = sample_block_len()
            # èƒ½æ”¾ä¸‹ L çš„ run åˆ—è¡¨
            candidate_runs = [(s, e) for (s, e) in runs if (e - s + 1) >= 1]
            if not candidate_runs:
                break

            # è‹¥ L å¤ªé•¿ï¼Œæ”¹çŸ­ä»¥ä¸è¶…è¿‡è¿˜éœ€æ”¾ç½®çš„ quota
            remain_need = quota - picked
            if L > remain_need:
                L = remain_need

            # åœ¨ candidate_runs ä¸­ç­›å‡ºèƒ½å®¹çº³ L çš„
            candidate_runs = [(s, e) for (s, e) in candidate_runs if (e - s + 1) >= L]
            if not candidate_runs:
                # è¿™ä¸ª L æ”¾ä¸ä¸‹ï¼Œå°è¯•æŠŠ L è°ƒåˆ°å½“å‰èƒ½æ”¾çš„æœ€å¤§ run é•¿åº¦
                max_len = 0
                best_runs = []
                for (s, e) in runs:
                    length = e - s + 1
                    if length > max_len:
                        max_len = length
                        best_runs = [(s, e)]
                    elif length == max_len and length > 0:
                        best_runs.append((s, e))
                if max_len == 0:
                    break
                # æ–° L æ˜¯ min(max_len, remain_need)
                L = min(max_len, remain_need)
                candidate_runs = best_runs

            # éšæœºæŒ‘ä¸€ä¸ª run
            ridx = np.random.randint(0, len(candidate_runs))
            s, e = candidate_runs[ridx]
            # åœ¨è¯¥ run å†…æŒ‘èµ·ç‚¹
            start = np.random.randint(s, e - L + 2)  # [s, e-L+1]
            end = start + L - 1

            # æ ‡è®°é€‰ä¸­åŒºåŸŸä¸ºâ€œå·²ä½¿ç”¨â€ï¼Œå¹¶è®°å½•
            plc[start:end + 1] = False
            chosen_rows.extend(range(start, end + 1))
            picked += L

        # å¦‚æœå—æ–¹å¼æ²¡æœ‰æ”¾æ»¡ï¼Œå›é€€ä¸ºéšæœºç‚¹å¡«è¡¥å‰©ä½™ quota
        remain = quota - picked
        if remain > 0:
            candidates = np.where(plc)[0]
            if candidates.size > 0:
                take = min(remain, candidates.size)
                extra = np.random.choice(candidates, take, replace=False)
                chosen_rows.extend(extra.tolist())
                picked += take

        return chosen_rows

        # end pick_blocks_in_one_column

    if per_col:
        # æŒ‰åˆ—æ”¾ç½®ï¼ˆäº’ä¸å¹²æ‰°ï¼‰
        for j in range(W):
            quota_j = int(col_quota[j])
            if quota_j <= 0:
                continue
            rows_j = pick_blocks_in_one_column(j, quota_j, placeable[:, j])
            # è®°å½•é€‰æ‹©
            for r in rows_j:
                chosen_rc.append((r, j))
                placeable[r, j] = False  # å ç”¨ï¼Œé¿å…åç»­é‡å¤
    else:
        # ç»Ÿä¸€æ± ï¼šé€åˆ—å¾ªç¯æ”¾ï¼Œç›´åˆ°ç”¨å®Œé…é¢ï¼ˆæ›´å‡è¡¡äº›ï¼‰
        remain = num_to_miss
        # å…ˆä¼°ä¸ªâ€œæ¯è½®ç›®æ ‡ chunk æ•°â€å¹¶åœ¨å„åˆ—å°è¯•ä¸€æ¬¡å—æ”¾ï¼Œéšåéšæœºè¡¥é½
        # è¿™é‡Œç®€åŒ–ä¸ºè½®è½¬å¼åˆ†é…ï¼Œæ¯åˆ—å°½é‡æ”¾ä¸€å—ï¼Œå†è½®è½¬
        col_order = list(range(W))
        while remain > 0 and placeable.any():
            progressed = False
            for j in col_order:
                if remain <= 0:
                    break
                # æ¯æ¬¡å°è¯•è‡³å°‘æ”¾ä¸€ä¸ªå—ï¼ˆé•¿åº¦ç”±é‡‡æ ·æˆ–åŒºé—´å†³å®šï¼Œä½†ä¸è¶…è¿‡ remainï¼‰
                quota_try = min(remain, sample_block_len())
                rows_j = pick_blocks_in_one_column(j, quota_try, placeable[:, j])
                if rows_j:
                    progressed = True
                    remain -= len(rows_j)
                    for r in rows_j:
                        chosen_rc.append((r, j))
                        placeable[r, j] = False
            if not progressed:
                break
        # è‹¥ä¾æ—§æœ‰å‰©ä½™ï¼Œæ•´å¼  sub éšæœºè¡¥é½
        if remain > 0:
            candidates = np.column_stack(np.where(placeable))  # (N,2) -> (r, j)
            if candidates.size > 0:
                take = min(remain, candidates.shape[0])
                idx = np.random.choice(np.arange(candidates.shape[0]), take, replace=False)
                extra = candidates[idx]
                for r, j in extra:
                    chosen_rc.append((r, j))
                    placeable[r, j] = False

    # å°† (row_offset, col) è½¬æˆåŸå§‹å…¨å±€è¡Œã€åˆ—ï¼Œå¹¶ç½® 0
    if not chosen_rc:
        raise RuntimeError("æ— æ³•æ”¾ç½®ä»»ä½•ç¼ºå¤±å—/ç‚¹ï¼ˆå¯èƒ½å¯ç”¨è§‚æµ‹æ®µè¿‡çŸ­/è¿‡å°‘ï¼‰ã€‚")

    chosen_rc = np.array(chosen_rc, dtype=int)
    global_rows = row_indices[chosen_rc[:, 0]]
    global_cols = chosen_rc[:, 1]
    masks[global_rows, global_cols] = 0

    return masks


# --- ä¸» Dataset ç±» (ä¿æŒç²¾ç®€ï¼Œä½¿ç”¨é¢„è®¡ç®—çš„ Mean/Std) ---
class Weather_Dataset(Dataset):
    def __init__(self, eval_length, full_data_with_nan, full_c_mask, train_mean, train_std, use_index,full_gt_mask,full_datetime_index,start_index):
        # full_data_with_nanï¼šåŒ…å« NaN çš„åŸå§‹æ•°æ®
        # full_c_maskï¼šåŸå§‹ observed_mask (1=è§‚æµ‹åˆ°, 0=åŸå§‹ç¼ºå¤±)

        self.eval_length = eval_length
        self.use_index = use_index
        # self.cut_length = [0] * len(use_index)
        self.full_datetime_index = full_datetime_index

        default_cut = eval_length - 1
        cut_length_list = [default_cut] * len(use_index)

        if len(use_index) > 0:
            # æ‰¾åˆ° use_index ä¸­å¯¹åº”å…¨å±€ start_index çš„ä½ç½®
            # np.where æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„ç´¢å¼•
            first_idx_in_set = np.where(np.array(use_index) == start_index)[0]

            if len(first_idx_in_set) > 0:
                # å°†è¯¥é›†åˆä¸­ç¬¬ä¸€ä¸ªçª—å£çš„ cut_length è®¾ç½®ä¸º 0 (æ— éœ€å±è”½)
                cut_length_list[first_idx_in_set[0]] = 0

        self.cut_length = cut_length_list  # ğŸ‘ˆ æ›´æ–° self.cut_length

        # 1. åº”ç”¨å½’ä¸€åŒ–å‚æ•° (åœ¨æ‰€æœ‰æ•°æ®ä¸Šåº”ç”¨ï¼Œä½†å‚æ•°åªæ¥è‡ªè®­ç»ƒé›†)

        # å°† NaN æ›¿æ¢ä¸º 0 (ä»¥ä¾¿å½’ä¸€åŒ–å…¬å¼ X - mean)
        c_data = np.nan_to_num(full_data_with_nan)


        K_total = c_data.shape[1]

        FEAT_DIM = 4
        if K_total % FEAT_DIM != 0:
            raise ValueError(
                f"K_total={K_total} ä¸èƒ½è¢« FEAT_DIM={FEAT_DIM} æ•´é™¤ï¼Œ"
                f"å½“å‰åˆ—å¸ƒå±€ä¸æ˜¯ [ç«™1çš„{FEAT_DIM}ç»´, ç«™2çš„{FEAT_DIM}ç»´, ...]ï¼Œè¯·å…ˆæ£€æŸ¥ get_all_data_and_indicesã€‚"
            )

        num_stations = K_total // FEAT_DIM

        # train_mean: (FEAT_DIM,)  â†’ é‡å¤åˆ°æ¯ä¸ªç«™ç‚¹ä¸Š â†’ (K_total,)
        mean_tile = np.tile(train_mean, num_stations)   # (K_total,)
        std_tile = np.tile(train_std, num_stations)     # (K_total,)

        mean_2d = mean_tile.reshape(1, K_total)         # (1, K_total)
        std_2d = std_tile.reshape(1, K_total)           # (1, K_total)

        self.full_observed_data = ((c_data - mean_2d) / std_2d) * full_c_mask

        # 2. å­˜å‚¨ Mask
        self.full_observed_data = self.full_observed_data.astype(np.float32)
        self.full_observed_mask = full_c_mask.astype(np.float32)

        self.full_gt_mask = full_gt_mask.astype(np.float32)
        self.full_hist_mask = np.copy(self.full_observed_mask)  # dummy

    def __getitem__(self, org_index):
        # ... (ä¸ä¹‹å‰ç‰ˆæœ¬ä¸€è‡´ï¼Œæ ¹æ® self.use_index æå–åˆ‡ç‰‡)
        
        index = self.use_index[org_index]  # æ»‘åŠ¨çª—å£çš„èµ·å§‹ä½ç½®
        current_datetime = self.full_datetime_index[index: index + self.eval_length]

        s = {
            # å½’ä¸€åŒ–åçš„æ•°æ®ï¼šæ¨¡å‹è¾“å…¥çš„æ ¸å¿ƒæ•°æ®ã€‚ å®ƒåŒ…å«æ‰€æœ‰ç‰¹å¾çš„å½’ä¸€åŒ–å€¼ï¼Œå…¶ä¸­åŸå§‹ç¼ºå¤±çš„ä½ç½®å·²ç»è¢«å¡«å……ä¸º 0ã€‚ (L, K)ï¼Œå³ï¼ˆLï¼Œ4ï¼‰
            "observed_data": self.full_observed_data[index: index + self.eval_length],
            # åŸå§‹è§‚æµ‹ Maskï¼šæ ‡è®°åŸå§‹æ•°æ®çš„è´¨é‡ã€‚ 1 è¡¨ç¤ºè¯¥ç‚¹åœ¨åŸå§‹æ•°æ®ä¸­æ˜¯è§‚æµ‹åˆ°çš„ï¼›0 è¡¨ç¤ºè¯¥ç‚¹åœ¨åŸå§‹æ•°æ®ä¸­æ˜¯ç¼ºå¤±çš„ã€‚(L, K)
            "observed_mask": self.full_observed_mask[index: index + self.eval_length],
            # è¯„ä¼°/æµ‹è¯•ç›®æ ‡ Maskï¼šå†³å®šè®­ç»ƒ/è¯„ä¼°çš„ç›®æ ‡ã€‚ 1 è¡¨ç¤ºè¯¥ç‚¹åœ¨è®­ç»ƒ/æµ‹è¯•æ—¶å·²çŸ¥ï¼ˆä½œä¸ºæ¨¡å‹è¾“å…¥ï¼‰ï¼›0 è¡¨ç¤ºè¯¥ç‚¹æ˜¯æ’å€¼ç›®æ ‡ï¼ˆåŸå§‹ç¼ºå¤±+äººé€ ç¼ºå¤±ï¼‰ã€‚ (L, K)
            "gt_mask": self.full_gt_mask[index: index + self.eval_length],
            # å†å²æ¨¡å¼ Maskï¼šPM2.5 æ•°æ®é›†é—ç•™çš„å…¼å®¹å­—æ®µã€‚ åœ¨ä½ çš„ç®€åŒ–ç‰ˆä¸­ï¼Œå®ƒåªæ˜¯ observed_mask çš„ä¸€ä¸ªå‰¯æœ¬ï¼Œä½œä¸º**è™šæ‹Ÿï¼ˆdummyï¼‰**è¾“å…¥ï¼Œå› ä¸ºä½ çš„æ¨¡å‹ç»“æ„å¯èƒ½éœ€è¦è¿™ä¸ªå­—æ®µã€‚
            "hist_mask": self.full_hist_mask[index: index + self.eval_length],
            # æ—¶é—´ç‚¹ç´¢å¼•ï¼šæ—¶é—´ç¼–ç è¾“å…¥ã€‚ åºåˆ—ä¸­æ¯ä¸ªæ—¶é—´ç‚¹çš„ç›¸å¯¹ç´¢å¼•ï¼Œé€šå¸¸æ˜¯ä» 0 åˆ° L-1ã€‚ç”¨äºç”Ÿæˆä½ç½®/æ—¶é—´åµŒå…¥ï¼ˆTime Embeddingï¼‰ã€‚ (L)
            "timepoints": np.arange(self.eval_length),
            # åˆ‡å‰²é•¿åº¦ï¼šæµ‹è¯•é›†è¯„ä¼°çš„è¾¹ç•Œã€‚ åœ¨æŸäº›æ•°æ®é›†ï¼ˆå¦‚ PM2.5ï¼‰ä¸­ï¼Œä¸ºäº†é¿å…æ»‘åŠ¨çª—å£é‡å¤è¯„ä¼°ï¼Œä¼šæ ‡è®°åºåˆ—å¼€å¤´æˆ–ç»“å°¾ä¸å‚ä¸è¯„ä¼°çš„é•¿åº¦ã€‚åœ¨ä½ çš„ç®€åŒ–ä»£ç ä¸­ï¼Œå®ƒæ€»æ˜¯ 0ã€‚ æ ‡é‡
            "cut_length": self.cut_length[org_index],
            "absolute_time": current_datetime.astype(str).tolist(),
        }
        return s

    def __len__(self):
        return len(self.use_index)

    
class Forecast_Weather_Dataset(Dataset):
    def __init__(self, eval_length, full_data_with_nan, full_c_mask, train_mean, train_std, use_index,full_datetime_index,start_index,horizon):
        # full_data_with_nanï¼šåŒ…å« NaN çš„åŸå§‹æ•°æ®
        # full_c_maskï¼šåŸå§‹ observed_mask (1=è§‚æµ‹åˆ°, 0=åŸå§‹ç¼ºå¤±)

        self.eval_length = eval_length
        self.use_index = use_index
        # self.cut_length = [0] * len(use_index)
        self.full_datetime_index = full_datetime_index
        self.horizon = horizon

        default_cut = eval_length - 1
        cut_length_list = [default_cut] * len(use_index)

        if len(use_index) > 0:
            # æ‰¾åˆ° use_index ä¸­å¯¹åº”å…¨å±€ start_index çš„ä½ç½®
            # np.where æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„ç´¢å¼•
            first_idx_in_set = np.where(np.array(use_index) == start_index)[0]

            if len(first_idx_in_set) > 0:
                # å°†è¯¥é›†åˆä¸­ç¬¬ä¸€ä¸ªçª—å£çš„ cut_length è®¾ç½®ä¸º 0 (æ— éœ€å±è”½)
                cut_length_list[first_idx_in_set[0]] = 0

        self.cut_length = cut_length_list  # ğŸ‘ˆ æ›´æ–° self.cut_length

        # 1. åº”ç”¨å½’ä¸€åŒ–å‚æ•° (åœ¨æ‰€æœ‰æ•°æ®ä¸Šåº”ç”¨ï¼Œä½†å‚æ•°åªæ¥è‡ªè®­ç»ƒé›†)

        # å°† NaN æ›¿æ¢ä¸º 0 (ä»¥ä¾¿å½’ä¸€åŒ–å…¬å¼ X - mean)
        c_data = np.nan_to_num(full_data_with_nan)

        K_total = c_data.shape[1]

        FEAT_DIM = 4
        if K_total % FEAT_DIM != 0:
            raise ValueError(
                f"K_total={K_total} ä¸èƒ½è¢« FEAT_DIM={FEAT_DIM} æ•´é™¤ï¼Œ"
                f"å½“å‰åˆ—å¸ƒå±€ä¸æ˜¯ [ç«™1çš„{FEAT_DIM}ç»´, ç«™2çš„{FEAT_DIM}ç»´, ...]ï¼Œè¯·å…ˆæ£€æŸ¥ get_all_data_and_indicesã€‚"
            )

        num_stations = K_total // FEAT_DIM

        # train_mean: (FEAT_DIM,)  â†’ é‡å¤åˆ°æ¯ä¸ªç«™ç‚¹ä¸Š â†’ (K_total,)
        mean_tile = np.tile(train_mean, num_stations)   # (K_total,)
        std_tile = np.tile(train_std, num_stations)     # (K_total,)

        mean_2d = mean_tile.reshape(1, K_total)         # (1, K_total)
        std_2d = std_tile.reshape(1, K_total)           # (1, K_total)

        self.full_observed_data = ((c_data - mean_2d) / std_2d) * full_c_mask

        # 2. å­˜å‚¨ Mask
        self.full_observed_data = self.full_observed_data.astype(np.float32)
        self.full_observed_mask = full_c_mask.astype(np.float32)

        # self.full_gt_mask = full_gt_mask.astype(np.float32)
        self.full_hist_mask = np.copy(self.full_observed_mask)  # dummy

    def __getitem__(self, org_index):
        # æ»‘åŠ¨çª—å£çš„èµ·å§‹ä½ç½®ï¼ˆåœ¨å…¨å±€æ—¶é—´è½´ä¸Šçš„ indexï¼‰
        index = self.use_index[org_index]
        L = self.eval_length

        # å½“å‰çª—å£å¯¹åº”çš„ç»å¯¹æ—¶é—´ï¼ˆDatetimeIndex -> strï¼‰
        current_datetime = self.full_datetime_index[index: index + L]

        # åŸå§‹è§‚æµ‹ mask çª—å£ (L, K_total)
        obs_mask_win = self.full_observed_mask[index: index + L]

        # é¢„æµ‹ä»»åŠ¡ä¸­ï¼šå†å²éƒ¨åˆ†ä½œä¸ºè¾“å…¥ï¼Œåé¢ horizon éƒ¨åˆ†ä½œä¸ºé¢„æµ‹ç›®æ ‡
        hist_len = L - self.horizon
        gt_mask_win = obs_mask_win.copy()
        gt_mask_win[hist_len:] = 0.0    # è¿™é‡Œç»Ÿä¸€å¯¹æ‰€æœ‰ç«™ç‚¹ã€æ‰€æœ‰ç‰¹å¾åšâ€œfutureâ€ä¸º0

        s = {
            # å½’ä¸€åŒ–åçš„æ•°æ®ï¼š(L, K_total)ï¼Œè¿™é‡Œçš„ K_total = num_stations * FEAT_DIM
            # æ¯ä¸€åˆ—å¯¹åº”ä¸€ä¸ªã€Œç«™ç‚¹-ç‰¹å¾ã€ç»„åˆï¼Œè€Œä¸æ˜¯ä»¥å‰çš„ã€Œå•ç«™ 4 ç»´ã€
            "observed_data": self.full_observed_data[index: index + L],

            # åŸå§‹è§‚æµ‹ Maskï¼š1=åŸå§‹è§‚æµ‹åˆ°ï¼Œ0=åŸå§‹ç¼ºå¤±ã€‚(L, K_total)
            "observed_mask": obs_mask_win,

            # è¯„ä¼°/æµ‹è¯•ç›®æ ‡ Maskï¼š1=ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œ0=ä½œä¸ºæ’è¡¥/é¢„æµ‹ç›®æ ‡ã€‚(L, K_total)
            "gt_mask": gt_mask_win,

            # å†å²æ¨¡å¼ Maskï¼šè¿™é‡Œä»ç„¶åªæ˜¯ observed_mask çš„ä¸€ä¸ªå‰¯æœ¬ï¼Œåš dummy å­—æ®µã€‚(L, K_total)
            "hist_mask": self.full_hist_mask[index: index + L],

            # æ—¶é—´ç‚¹ç´¢å¼•ï¼š0..L-1ï¼Œç”¨äºåš time embeddingã€‚(L,)
            "timepoints": np.arange(L),

            # åˆ‡å‰²é•¿åº¦ï¼šä¿æŒä½ åŸæ¥çš„é€»è¾‘ï¼ˆä¸€èˆ¬æ˜¯ L-1 æˆ– 0ï¼Œçœ‹ä½ å‰é¢æ€ä¹ˆè®¾çš„ï¼‰
            "cut_length": self.cut_length[org_index],

            # ç»å¯¹æ—¶é—´å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆæ–¹ä¾¿ä½ åå¤„ç†æˆ–ç”»å›¾ï¼‰
            "absolute_time": current_datetime.astype(str).tolist(),
        }
        return s

    def __len__(self):
        return len(self.use_index)


# --- Dataloader è·å–å‡½æ•° (æ›´æ–°è°ƒç”¨æµç¨‹) ---
def get_dataloader(batch_size, device, eval_length=36):
    # 1. è·å–æ‰€æœ‰æ•°æ®å’Œç´¢å¼•åˆ’åˆ†
    full_data_with_nan, full_c_mask, full_datetime_index,train_indices, valid_indices, test_indices,train_start_idx,valid_start_idx,test_start_idx = get_all_data_and_indices(Dir_PATH, eval_length)

    train_gt_mask = full_c_mask
    valid_gt_mask = create_full_gt_mask2(full_c_mask,valid_indices, missing_ratio=0.3, seed=66)
    test_gt_mask = create_full_gt_mask(full_c_mask,test_indices, missing_ratio=0.2, seed=520)

    # 2. ä»…åœ¨è®­ç»ƒé›†ç´¢å¼•ä¸Šè®¡ç®— Mean å’Œ Std (æœ€ä½³å®è·µ)
    train_mean, train_std = calculate_train_mean_std(
        full_data_with_nan, full_c_mask, train_indices, eval_length
    )

    # 3. åˆå§‹åŒ–æ•°æ®é›† (æ‰€æœ‰æ•°æ®é›†å…±äº« train_mean/train_std)
    train_dataset = Weather_Dataset(
        eval_length, full_data_with_nan, full_c_mask, train_mean, train_std, train_indices, train_gt_mask,full_datetime_index,train_start_idx
    )
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, num_workers=8, shuffle=True,pin_memory=True
    )

    valid_dataset = Weather_Dataset(
        eval_length, full_data_with_nan, full_c_mask, train_mean, train_std, valid_indices, valid_gt_mask,full_datetime_index,valid_start_idx
    )
    valid_loader = DataLoader(
        valid_dataset, batch_size=batch_size, num_workers=8, shuffle=False,pin_memory=True
    )

    test_dataset = Weather_Dataset(
        eval_length, full_data_with_nan, full_c_mask, train_mean, train_std, test_indices, test_gt_mask,full_datetime_index,test_start_idx
    )
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, num_workers=8, shuffle=False,pin_memory=True
    )

    # ä¼ é€’ Scalers
    scaler = torch.from_numpy(train_std).to(device).float()
    mean_scaler = torch.from_numpy(train_mean).to(device).float()

    return train_loader, valid_loader, test_loader, scaler, mean_scaler,full_datetime_index


def get_forecast_dataloader(batch_size, device, eval_length=36):
    # 1. è·å–æ‰€æœ‰æ•°æ®å’Œç´¢å¼•åˆ’åˆ†
    full_data_with_nan, full_c_mask, full_datetime_index,train_indices, valid_indices, test_indices,train_start_idx,valid_start_idx,test_start_idx = get_all_data_and_indices(Dir_PATH, eval_length)

    # train_gt_mask = full_c_mask
    # valid_gt_mask = create_full_gt_mask2(full_c_mask,valid_indices, missing_ratio=0.3, seed=66)
    # test_gt_mask = create_full_gt_mask(full_c_mask,test_indices, missing_ratio=0.2, seed=520)

    # 2. ä»…åœ¨è®­ç»ƒé›†ç´¢å¼•ä¸Šè®¡ç®— Mean å’Œ Std (æœ€ä½³å®è·µ)
    train_mean, train_std = calculate_train_mean_std(
        full_data_with_nan, full_c_mask, train_indices, eval_length
    )

    # 3. åˆå§‹åŒ–æ•°æ®é›† (æ‰€æœ‰æ•°æ®é›†å…±äº« train_mean/train_std)
    train_dataset = Forecast_Weather_Dataset(
        eval_length, full_data_with_nan, full_c_mask, train_mean, train_std, train_indices,full_datetime_index,train_start_idx,horizon=1
    )
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, num_workers=8, shuffle=True,pin_memory=True
    )

    valid_dataset = Forecast_Weather_Dataset(
        eval_length, full_data_with_nan, full_c_mask, train_mean, train_std, valid_indices,full_datetime_index,valid_start_idx,horizon=1
    )
    valid_loader = DataLoader(
        valid_dataset, batch_size=batch_size, num_workers=8, shuffle=False,pin_memory=True
    )

    test_dataset = Forecast_Weather_Dataset(
        eval_length, full_data_with_nan, full_c_mask, train_mean, train_std, test_indices,full_datetime_index,test_start_idx,horizon=1
    )
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, num_workers=8, shuffle=False,pin_memory=True
    )

    # ä¼ é€’ Scalers
    scaler = torch.from_numpy(train_std).to(device).float()
    mean_scaler = torch.from_numpy(train_mean).to(device).float()

    return train_loader, valid_loader, test_loader, scaler, mean_scaler,full_datetime_index